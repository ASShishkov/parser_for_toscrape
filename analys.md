Задание: 
Сбор данных с сайта https://quotes.toscrape.com/

Результаты:

  JSON файл с собранными данными:
    В папке с проектом есть два файла. 1 - Все собранные цитаты с сайта, 2 - Уникальные цитаты

  MD файл с анализом задачи, включающий:

Что было сделано:
  Был создан проект "parser_for_toscrape", собирающий цитаты с сайта https://quotes.toscrape.com

  Использованы библиотеки:
    requests(для отправки GET и POST HTTP-запросов для получения данных с сайта)
    BeautifulSoup, для парсинга HTML и XML документов
    json для работы с форматом JSON
  
  Откуда были получены данные:
    Данные собираются с сайта https://quotes.toscrape.com

  Как осуществлялся сбор:
    Проблема в том, что нет общей структуры и общего списка цитат, как и тегов, 
    поэтому в начале собираются все тэги и сохраняются в файл, и потом по ним собираются все статьи.
    Но так как к одному тегу могут пренадлежать несколько цитат, дубли можно исключить, 
    оставив только уникальные цитаты.

  Почему был выбран тот или иной метод/инструмент, а не другой:
    Было рассмотрено четыре метода: 
    BeautifulSoup(самый лёгкий и простой для статических страниц)
    Selenium(более для динамических страниц)
    API(лучший способ, если сайт даёт API)
    Scrapy(для многопоточных парсеров)
  Выбран был самый простой, но достаточный для выполнения задачи метод - BeautifulSoup.

  Ссылка на репозиторий с кодом на GitHub